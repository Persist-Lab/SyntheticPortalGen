{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2df5cb-ce5d-4db4-a119-568675dee6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"<path>\")\n",
    "\n",
    "from tqdm.auto import tqdm \n",
    "from pprint import pprint\n",
    "import pandas as pd \n",
    "\n",
    "import load_llms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1f651-3dcc-4b5c-8ade-94761d7584c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_llms import load_llm_in_4bit\n",
    "'''\n",
    "Locally stored LLM paths\n",
    "'''\n",
    "llm_paths = {\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4b3bd-38bf-4d21-80eb-ec17f321f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "    prompts = pd.read_csv(\"<path>\")[['prompt']]\n",
    "    return prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180b2be-3aeb-4efb-b78f-132eb5658d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(x):\n",
    "    \n",
    "    instruction = '''Pretend you are a medical patient. Write a message to your doctor using the message prompt. \n",
    "    \n",
    "    ### Rules ###\n",
    "    - Assume the doctor you are messaging has been your physician for years. It is permissable to speak informally when appropriate. \n",
    "    - Do not restate the prompt in the message.\n",
    "    - You may add additional health context (e.g. symptoms or medications) to the message as needed. \n",
    "    '''\n",
    "\n",
    "    chat = []\n",
    "    ### Add test sample at the end ###\n",
    "    chat.append({'role': 'user', 'content': f'''{instruction}\n",
    "    \n",
    "        Prompt: {x['prompt']}\n",
    "\n",
    "        Patient Message:'''})\n",
    "    \n",
    "    return chat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd90f8-0ecd-4765-a4eb-c65d46d98f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse the pipeline outputs ### \n",
    "def parse_outputs(x):\n",
    "    return x[0]['generated_text'][-1]['content']\n",
    "def parse_prompts(x):\n",
    "    return x[-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed01a7d-d83c-4afc-9cfe-80eb11980b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "def run(model_str):\n",
    "    \n",
    "    pipe, tokenizer, model = load_llms.load_llm_in_4bit(llm_paths[model_str])\n",
    "    prompts = prep_data()\n",
    "    \n",
    "    ### Shuffle inputs before each generation ### \n",
    "    all_prompts = [create_prompt(prompts.iloc[x]) for x in range(len(prompts))]\n",
    "    outputs = pipe(all_prompts, temperature = 0.75, do_sample=True, batch_size = 6)\n",
    "    parsed_outputs = [{'message':parse_outputs(x), 'prompt':parse_prompts(p)} for x,p in zip(outputs, all_prompts)]\n",
    "    \n",
    "    pd.DataFrame(parsed_outputs).to_csv(\"<path>\")\n",
    "    \n",
    "    ### Clean up models to load another ### \n",
    "    pipe = None\n",
    "    model = None \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
